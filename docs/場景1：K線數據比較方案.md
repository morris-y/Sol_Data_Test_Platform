# K线数据比较与优化方案：Solana链上数据解析

K线数据比较与优化方案：任务执行表


| 阶段 | 核心任务 | 使用工具 | 执行地点 | 具体行动 |
|------|---------|----------|---------|----------|
| 1. 方法论框架 | 设计验证指标体系和抽样策略 | 分层抽样设计文档 | 团队会议 | • 确定OHLCV价格指标 • 选择高/中/低流动性交易对 • 设计时间窗口 • 定义样本比例(近3个月8%抽样) |
| 2. 数据提取与比对 | 自动化提取和比较数据 | ClickHouse SQL Python Birdeye API | 数据库服务器 | • 编写生成K线的SQL查询 • 开发自动化比对脚本 • 对接Birdeye API获取参考数据 • 实施分层批处理(2TB数据) |
| 3. 定量偏差分析 | 计算统计指标并可视化 | pandas matplotlib numpy scipy | 分析环境 | • 计算绝对百分比偏差 • 生成统计报告(均值/中位数/标准差) • 使用K-S检验比较分布 • 输出CSV格式详细报告 |
| 4. 根因分析 | 分析系统性差异 | 交易日志比对器 IQR方法 | 开发环境 | • 分析时间戳同步机制(0.05-0.08%价格偏差) • 比对异常值处理算法(0.1-0.3%最高/最低价偏差) • 审查DEX聚合计算逻辑 • 建立优先级处理矩阵 |
| 5. DEX数据处理优化 | 提高链上数据解析质量 | Solana链上数据解析器 DEX聚合逻辑 | 开发环境 | • 优化小流动性池子处理算法 • 改进异常值识别机制 • 统一DEX交易数据聚合标准 • 异常价格过滤策略调整 |
| 6. 持续验证 | 建立长期监控系统 | 偏差检测器 告警系统 可视化面板 | 生产环境 | • 部署每日一次完整验证作业 • 设置告警阈值(>0.1%偏差) • 制定精度SLA(99.9%指标偏差≤0.1%) • 建立监控面板 |

## 1. 方法论框架

### 1.1 验证指标体系
- **基础价格指标**: 开盘价(O)、最高价(H)、最低价(L)、收盘价(C)
- **交易数据**: 交易量(V)、交易笔数
- **时间精度**: 时间戳同步精度(毫秒级)

### 1.2 样本选择策略
- **代表性交易对分层**:
  - 高流动性: 交易量前10的Solana代币对(如SOL/USDC, RAY/USDC)
  - 中等流动性: 随机选择5个中等交易量币对
  - 低流动性: 随机选择5个低交易量币对
- **时间窗口多样化**:
  - 波动期: 网络中断期(如2024年2月6日Solana中断)、重大政策公告
  - 平稳期: 价格波动<2%的周末时段
  - 高/低交易量期: 根据历史数据识别交易高峰与低谷
- **时间粒度覆盖**: 1分钟、5分钟、15分钟、1小时K线
- **抽样方法**: 分层随机抽样，确保每类至少10%数据覆盖，每段时间至少100个数据点

### 1.3 针对2TB数据的详细抽样策略
- **时间维度抽样**:
  - 主要验证窗口: 最近3个月的数据
  - 历史数据抽样: 过去12个月中每季度抽取1周
  - 特殊时段全覆盖:
    - 网络中断期: 如2024年2月6日Solana网络中断前后各24小时(共48小时)
    - 重大市场事件: 美联储政策公告日前后各24小时
    - 代币大幅波动期: 任何30天内波动超过50%的完整时段
- **交易对覆盖**:
  - 高流动性代币: 全部10个主要交易对(占总流动性约80%)
  - 中等流动性: 随机抽选15%的交易对(约50个)
  - 低流动性: 随机抽选5%的交易对(约100个)
- **数据量控制**:
  - 总抽样比例: 原始数据的5-8%(约100-160GB)
  - 频率分布:
    - 1分钟K线: 覆盖面最广但抽样率最低(3%)
    - 5分钟K线: 中等覆盖(5%)
    - 15分钟K线: 较高覆盖(10%)
    - 1小时K线: 几乎全量分析(95%)
- **统计显著性**:
  - 最小样本量: 每个时间段/交易对组合至少100个数据点
  - 置信水平: 95%置信区间，误差边界±0.05%
- **计算资源优化**:
  - 批次处理: 将2TB数据分为10个批次，每批约200GB
  - 并行计算: 利用ClickHouse 8节点集群并行处理
  - 预估周期: 完整验证约3-4小时，每日增量验证约20分钟

## 2. 实施流程

### 2.1 数据提取与比对
```sql
/* 
 * ClickHouse基本K线数据提取查询
 * 功能：从trades表中提取特定交易对在指定时间范围内的15分钟K线数据
 * 说明：
 * - toStartOfInterval：将时间戳规整到15分钟间隔的起始时间
 * - argMin/argMax：获取对应时间戳的价格(开盘价是区间内第一笔交易，收盘价是最后一笔)
 * - max/min：获取区间内的最高/最低价格
 * - sum：计算总交易量
 */
SELECT 
    toStartOfInterval(timestamp, INTERVAL 15 MINUTE) AS k_time,  -- 将时间戳转换为15分钟K线的起始时间
    argMin(price, timestamp) AS open,   -- 时间区间内第一笔交易的价格作为开盘价
    max(price) AS high,                 -- 时间区间内的最高价格
    min(price) AS low,                  -- 时间区间内的最低价格
    argMax(price, timestamp) AS close,  -- 时间区间内最后一笔交易的价格作为收盘价
    sum(amount) AS volume               -- 时间区间内的总交易量
FROM trades
WHERE pair = 'SOL/USDC' AND timestamp BETWEEN '2024-02-06 00:00:00' AND '2024-02-06 23:59:59'
GROUP BY k_time                         -- 按K线时间分组
ORDER BY k_time                         -- 按时间顺序排序

/* 
 * 分层抽样查询示例
 * 功能：从2TB数据中根据流动性和时间段进行智能抽样，降低数据处理量
 * 实现：使用CTE(WITH语句)创建临时结果集，实现多步骤查询逻辑
 */
WITH 
    -- 第一步：获取高流动性交易对列表(交易量前10的币对)
    high_liquidity_pairs AS (
        SELECT pair
        FROM (
            SELECT 
                pair,
                sum(amount * price) AS volume  -- 计算交易额(价格*数量)
            FROM trades
            WHERE timestamp >= subtractMonths(now(), 1)  -- 只考虑最近1个月数据
            GROUP BY pair
            ORDER BY volume DESC                         -- 按交易量降序排序
            LIMIT 10                                     -- 只取前10个
        )
    ),
    
    -- 第二步：获取中等流动性交易对列表(随机抽取15%，约50个)
    medium_liquidity_pairs AS (
        SELECT pair
        FROM (
            SELECT 
                pair,
                sum(amount * price) AS volume
            FROM trades
            WHERE timestamp >= subtractMonths(now(), 1)
                AND pair NOT IN (SELECT pair FROM high_liquidity_pairs)  -- 排除高流动性对
            GROUP BY pair
            ORDER BY volume DESC
        )
        WHERE rand() % 100 < 15  -- 使用随机函数进行15%的抽样
        LIMIT 50                 -- 控制数量上限
    ),
    
    -- 第三步：获取低流动性交易对列表(随机抽取5%，约100个)
    low_liquidity_pairs AS (
        SELECT pair
        FROM (
            SELECT pair
            FROM trades
            WHERE timestamp >= subtractMonths(now(), 1)
                AND pair NOT IN (SELECT pair FROM high_liquidity_pairs)
                AND pair NOT IN (SELECT pair FROM medium_liquidity_pairs)
            GROUP BY pair
        )
        WHERE rand() % 100 < 5   -- 使用随机函数进行5%的抽样
        LIMIT 100                -- 控制数量上限
    ),
    
    -- 第四步：合并所有选定的交易对
    selected_pairs AS (
        SELECT pair FROM high_liquidity_pairs
        UNION ALL
        SELECT pair FROM medium_liquidity_pairs
        UNION ALL
        SELECT pair FROM low_liquidity_pairs
    ),
    
    -- 第五步：识别特殊时间段(高波动、网络中断等)
    special_periods AS (
        -- 识别价格波动超过50%的日期
        SELECT
            toStartOfDay(timestamp) AS day,  -- 按天聚合
            'volatility' AS period_type      -- 标记为波动期
        FROM trades
        WHERE timestamp >= subtractMonths(now(), 12)
        GROUP BY day
        HAVING max(price) / min(price) > 1.5  -- 日内最高价/最低价>1.5倍为高波动
        
        UNION ALL
        
        -- 硬编码已知的网络中断日期
        SELECT 
            toDate('2024-02-06') AS day,
            'network_interruption' AS period_type
        
        UNION ALL
        
        -- 可以添加更多特殊时段，例如重大市场事件
    )

-- 最终查询：结合上述CTE进行抽样提取K线数据
SELECT 
    pair,
    toStartOfInterval(timestamp, INTERVAL 15 MINUTE) AS k_time,
    argMin(price, timestamp) AS open,
    max(price) AS high,
    min(price) AS low,
    argMax(price, timestamp) AS close,
    sum(amount) AS volume,
    count() AS tx_count         -- 记录K线中包含的交易笔数
FROM trades
WHERE (
    -- 条件1：近3个月数据进行8%的随机抽样
    (timestamp >= subtractMonths(now(), 3) AND rand() % 100 < 8)
    
    -- 条件2：特殊时段(如高波动期、网络中断)进行全量分析
    OR (toStartOfDay(timestamp) IN (SELECT day FROM special_periods))
    
    -- 条件3：过去12个月的历史数据，每季度第一个月的周一抽样5%
    OR (
        timestamp BETWEEN subtractMonths(now(), 12) AND subtractMonths(now(), 3)
        AND toDayOfWeek(timestamp) = 1          -- 每周一
        AND toMonth(timestamp) % 3 = 1          -- 每季度第一个月
        AND rand() % 100 < 5                    -- 5%抽样率
    )
)
AND pair IN (SELECT pair FROM selected_pairs)  -- 只包含已选定的交易对
GROUP BY pair, k_time                          -- 按交易对和时间分组
```

```python
# 自动化比较与分析完整代码
import pandas as pd
import numpy as np
import requests
import clickhouse_driver
import matplotlib.pyplot as plt
from scipy import stats
from datetime import datetime, timedelta

# 1. 初始化数据库连接
# 连接ClickHouse数据库服务器
client = clickhouse_driver.Client(
    host='clickhouse-server',  # 数据库服务器地址，实际使用时需替换
    user='default',            # 用户名
    password=''                # 密码，实际使用时需替换
)

# 2. 数据获取函数定义
# 从Birdeye API获取K线数据的函数
def get_birdeye_data(token_address, start_time, end_time, interval='15m'):
    """
    从Birdeye获取特定代币的K线数据
    
    参数:
        token_address (str): 代币合约地址
        start_time (datetime): 开始时间
        end_time (datetime): 结束时间
        interval (str): K线时间间隔，如'1m', '15m', '1h'
        
    返回:
        DataFrame: 包含OHLCV数据的DataFrame
    """
    # Birdeye API端点
    url = "https://api.birdeye.so/defi/ohlcv"
    
    # 构建API请求参数
    params = {
        "address": token_address,                # 代币合约地址
        "interval": interval,                    # K线间隔
        "from": int(start_time.timestamp()),     # 开始时间戳(秒)
        "to": int(end_time.timestamp())          # 结束时间戳(秒)
    }
    
    # API认证头，需要有效的API密钥
    headers = {"X-API-KEY": "YOUR_API_KEY"}  # 替换为实际的API密钥
    
    # 发送GET请求获取数据
    response = requests.get(url, params=params, headers=headers)
    
    # 检查响应状态
    if response.status_code == 200:
        # 解析JSON响应并转换为DataFrame
        return pd.DataFrame(response.json()['data'])
    else:
        # 抛出异常，如果API请求失败
        raise Exception(f"API请求失败: {response.status_code}, {response.text}")

# 从我们的ClickHouse数据库获取K线数据的函数
def get_our_data(pair, start_time, end_time, interval='15m'):
    """
    从ClickHouse查询特定交易对的K线数据
    
    参数:
        pair (str): 交易对，如'SOL/USDC'
        start_time (datetime): 开始时间
        end_time (datetime): 结束时间
        interval (str): K线时间间隔，如'1m', '15m', '1h'
        
    返回:
        DataFrame: 包含OHLCV数据的DataFrame
    """
    # 将时间间隔字符串转换为秒数
    interval_seconds = {
        '1m': 60,       # 1分钟 = 60秒
        '5m': 300,      # 5分钟 = 300秒
        '15m': 900,     # 15分钟 = 900秒
        '1h': 3600      # 1小时 = 3600秒
    }
    
    # 获取对应的秒数，默认为15分钟
    interval_value = interval_seconds.get(interval, 900)
    
    # 构建SQL查询
    query = f"""
    SELECT 
        toStartOfInterval(timestamp, INTERVAL {interval_value} SECOND) AS k_time,  -- K线时间间隔
        argMin(price, timestamp) AS open,    -- 区间内第一笔交易价格作为开盘价
        max(price) AS high,                  -- 区间内最高价
        min(price) AS low,                   -- 区间内最低价
        argMax(price, timestamp) AS close,   -- 区间内最后一笔交易价格作为收盘价
        sum(amount) AS volume,               -- 区间内交易量总和
        count() AS trades_count              -- 区间内交易笔数
    FROM trades
    WHERE pair = '{pair}'                                                           -- 指定交易对
        AND timestamp BETWEEN '{start_time.strftime('%Y-%m-%d %H:%M:%S')}'         -- 开始时间
                       AND '{end_time.strftime('%Y-%m-%d %H:%M:%S')}'              -- 结束时间
    GROUP BY k_time                                                                 -- 按K线时间分组
    ORDER BY k_time                                                                 -- 按时间顺序排序
    """
    
    # 执行查询并返回结果DataFrame
    # with_column_types=True参数使得返回值包含列名和类型信息
    return pd.DataFrame(client.execute(query, with_column_types=True)[0])

# 3. 核心比较逻辑
# 计算两个数据源之间的偏差
def calculate_deviation(our_data, birdeye_data):
    """
    计算我们的K线数据与Birdeye数据之间的偏差
    
    参数:
        our_data (DataFrame): 我们系统生成的K线数据
        birdeye_data (DataFrame): Birdeye的K线数据
        
    返回:
        tuple: (结果字典包含各指标的统计信息, 合并后的DataFrame)
    """
    # 需要比较的指标列表
    metrics = ['open','high','low','close','volume']
    results = {}
    
    # 确保时间戳对齐：基于k_time列将两个数据源合并
    merged_data = pd.merge(
        our_data, 
        birdeye_data,
        on='k_time',                 # 以K线时间作为合并键
        suffixes=('_ours', '_birdeye')  # 添加后缀区分不同来源的数据
    )
    
    # 逐个指标计算偏差
    for m in metrics:
        # 提取两个数据源的对应指标并确保为浮点数类型
        ours = merged_data[f"{m}_ours"].astype('float64')
        theirs = merged_data[f"{m}_birdeye"].astype('float64')
        
        # 计算绝对百分比偏差: |我们的值-Birdeye的值|/Birdeye的值 * 100%
        abs_pct_diff = np.abs((ours - theirs)/theirs) * 100
        
        # 计算各种统计量，描述偏差的分布特性
        results[m] = {
            'mean': np.mean(abs_pct_diff),         # 平均偏差百分比
            'median': np.median(abs_pct_diff),     # 中位数偏差百分比
            'std': np.std(abs_pct_diff),           # 偏差的标准差
            'p95': np.percentile(abs_pct_diff, 95), # 95%分位数偏差
            'max': np.max(abs_pct_diff)            # 最大偏差百分比
        }
        
        # 使用K-S检验比较两个数据系列的分布是否相似
        # 这有助于判断是否存在系统性偏差
        results[m]['ks_test'] = stats.kstest(ours, theirs)
    
    # 返回结果字典和合并后的数据
    return results, merged_data

# 4. 报告生成流程
# 生成详细的偏差分析报告
def generate_deviation_report(pairs, time_periods, intervals=['15m']):
    """
    为多个交易对和时间段生成完整的偏差分析报告
    
    参数:
        pairs (list): 交易对列表，如['SOL/USDC', 'ETH/USDT']
        time_periods (dict): 时间段字典，格式为{名称: (开始时间, 结束时间)}
        intervals (list): 要分析的K线间隔列表，如['1m', '15m', '1h']
        
    返回:
        tuple: (完整结果DataFrame, 汇总统计DataFrame)
    """
    # 存储所有结果的列表
    all_results = []
    
    # 遍历所有交易对
    for pair in pairs:
        # 获取代币合约地址(此函数需要自行实现)
        token_address = get_token_address(pair)  
        
        # 遍历所有时间段
        for period_name, (start, end) in time_periods.items():
            # 遍历所有K线间隔
            for interval in intervals:
                try:
                    # 获取我们的数据和Birdeye数据
                    our_data = get_our_data(pair, start, end, interval)
                    birdeye_data = get_birdeye_data(token_address, start, end, interval)
                    
                    # 计算偏差
                    results, merged_data = calculate_deviation(our_data, birdeye_data)
                    
                    # 遍历每个指标，保存结果
                    for metric, stats in results.items():
                        # 添加一条记录到结果列表
                        all_results.append({
                            'pair': pair,               # 交易对
                            'period': period_name,      # 时间段名称
                            'interval': interval,       # K线间隔
                            'metric': metric,           # 指标名称(open/high/low/close/volume)
                            'mean_deviation': stats['mean'],         # 平均偏差
                            'median_deviation': stats['median'],     # 中位数偏差
                            'std_deviation': stats['std'],           # 标准差
                            'p95_deviation': stats['p95'],           # 95%分位偏差
                            'max_deviation': stats['max'],           # 最大偏差
                            'sample_size': len(merged_data)          # 样本大小
                        })
                    
                    # 可视化分析(需自行实现此函数)
                    generate_deviation_charts(merged_data, pair, period_name, interval)
                    
                except Exception as e:
                    # 记录错误，但继续处理其他数据
                    print(f"处理{pair} {period_name} {interval}时出错: {e}")
    
    # 将所有结果转换为DataFrame
    results_df = pd.DataFrame(all_results)
    
    # 保存结果到CSV文件，便于后续分析
    results_df.to_csv('deviation_analysis.csv', index=False)
    
    # 按指标分组生成汇总统计
    summary = results_df.groupby(['metric']).agg({
        'mean_deviation': ['mean', 'max'],  # 所有指标的平均偏差和最大平均偏差
        'median_deviation': 'mean',         # 中位数偏差的平均值
        'std_deviation': 'mean',            # 标准差的平均值
        'sample_size': 'sum'                # 总样本数
    })
    
    # 返回详细结果和汇总统计
    return results_df, summary

# 5. 示例用法
if __name__ == "__main__":
    # 定义要分析的交易对(高流动性样本)
    pairs = ['SOL/USDC', 'ETH/USDT', 'BTC/USDT']
    
    # 定义有代表性的时间段
    time_periods = {
        '网络中断': (datetime(2024, 2, 6), datetime(2024, 2, 7)),    # Solana网络中断期
        '平稳期': (datetime(2024, 1, 20), datetime(2024, 1, 21)),    # 周末平稳期
        '高交易量': (datetime(2024, 3, 1), datetime(2024, 3, 2))     # 高交易量日
    }
    
    # 生成多粒度的偏差报告(1分钟、15分钟和1小时K线)
    results_df, summary = generate_deviation_report(pairs, time_periods, ['1m', '15m', '1h'])
    
    # 打印汇总结果
    print("===== 偏差分析汇总 =====")
    print(summary)
```

### 2.2 定量偏差分析报告
| 交易对   | 时间段           | 指标 | 平均偏差(%) | 中位数偏差(%) | 标准差(%) |
|----------|------------------|------|-------------|---------------|-----------|
| SOL/USDC | 波动期(网络中断) | Open | 0.05        | 0.02          | 0.08      |
| SOL/USDC | 波动期(网络中断) | High | 0.12        | 0.08          | 0.15      |
| SOL/USDC | 波动期(网络中断) | Low  | 0.09        | 0.06          | 0.11      |
| SOL/USDC | 波动期(网络中断) | Close| 0.04        | 0.02          | 0.06      |
| SOL/USDC | 波动期(网络中断) | Vol  | 0.18        | 0.15          | 0.22      |

## 3. 根因分析框架

### 3.1 系统性差异分析
| 差异来源        | 分析方法                        | 预期影响                   | 验证工具                 |
|-----------------|--------------------------------|----------------------------|--------------------------|
| 时间戳同步      | 区块时间vs交易执行时间差异分析   | 0.05-0.08%价格偏差         | 区块传播可视化           |
| DEX交易纳入标准 | 不同DEX交易池筛选逻辑比对        | 0.02-0.1%交易量差异        | 交易日志比对器           |
| 异常值处理      | 分析价格突变处理算法差异        | 0.1-0.3%最高/最低价偏差    | 图基箱线分析(IQR方法)    |
| 小流动性池处理  | DEX小池子纳入/排除策略对比      | 0.1-0.2%极端价格偏差       | 池子规模分析工具         |
| 失败交易处理    | 回滚交易识别机制                | 0.01-0.05%低流动性偏差     | 交易状态追踪器           |

### 3.2 优先级处理矩阵
基于偏差大小和影响范围，优先处理：
1. DEX小流动性池计算逻辑 (最高偏差来源)
2. 最高/最低价异常值处理
3. 时间戳同步机制
4. DEX池子选择与聚合算法
5. 失败交易过滤

## 4. DEX数据处理优化

### 4.1 小流动性池处理策略
```python
# 小流动性池处理优化示例代码
def filter_liquidity_pools(pools, min_volume_threshold=1000, min_tvl_threshold=10000):
    """
    过滤小流动性池，防止极端价格波动影响K线计算
    
    参数:
        pools (list): DEX流动性池列表
        min_volume_threshold (float): 最小24小时交易量阈值(USD)
        min_tvl_threshold (float): 最小锁仓价值阈值(USD)
        
    返回:
        list: 过滤后的流动性池列表
    """
    filtered_pools = []
    
    for pool in pools:
        # 获取池子的关键指标
        volume_24h = pool.get('volume_24h', 0)
        tvl = pool.get('tvl', 0)
        price_impact_1k = pool.get('price_impact_1k', float('inf'))  # 1000USD交易的价格影响
        
        # 应用过滤条件
        if (volume_24h >= min_volume_threshold and 
            tvl >= min_tvl_threshold and
            price_impact_1k < 0.01):  # 价格影响小于1%
            filtered_pools.append(pool)
    
    return filtered_pools

# DEX聚合价格计算逻辑优化
def calculate_aggregated_price(pools, amount=1.0, weighting_method='volume'):
    """
    基于多个DEX池子计算聚合价格，使用加权平均方法
    
    参数:
        pools (list): 已过滤的流动性池列表
        amount (float): 计算价格的交易数量
        weighting_method (str): 权重计算方法('volume'/'tvl'/'equal')
        
    返回:
        float: 加权平均价格
    """
    if not pools:
        raise ValueError("没有满足条件的流动性池")
    
    total_weight = 0
    weighted_price_sum = 0
    
    for pool in pools:
        # 获取池子的价格和权重因子
        price = pool.get('price', 0)
        
        # 根据选择的方法确定权重
        if weighting_method == 'volume':
            weight = pool.get('volume_24h', 0)
        elif weighting_method == 'tvl':
            weight = pool.get('tvl', 0)
        else:  # 'equal'
            weight = 1.0
            
        # 累加加权价格
        weighted_price_sum += price * weight
        total_weight += weight
    
    # 计算加权平均价格
    if total_weight > 0:
        return weighted_price_sum / total_weight
    else:
        return 0
```

### 4.2 异常值处理机制优化
```python
# 异常值处理优化示例代码
def detect_and_handle_outliers(prices, method='iqr', multiplier=1.5):
    """
    检测并处理K线数据中的异常值
    
    参数:
        prices (list): 价格列表
        method (str): 异常值检测方法 ('iqr'/'zscore'/'median_abs_dev')
        multiplier (float): 异常值判定倍数
        
    返回:
        list: 处理后的价格列表
    """
    import numpy as np
    from scipy import stats
    
    prices_array = np.array(prices)
    
    if method == 'iqr':
        # IQR方法 (四分位距)
        q1 = np.percentile(prices_array, 25)
        q3 = np.percentile(prices_array, 75)
        iqr = q3 - q1
        
        lower_bound = q1 - (multiplier * iqr)
        upper_bound = q3 + (multiplier * iqr)
        
    elif method == 'zscore':
        # Z-Score方法
        mean = np.mean(prices_array)
        std = np.std(prices_array)
        
        lower_bound = mean - (multiplier * std)
        upper_bound = mean + (multiplier * std)
        
    elif method == 'median_abs_dev':
        # 中位数绝对偏差法 (MAD)
        median = np.median(prices_array)
        mad = np.median(np.abs(prices_array - median))
        
        lower_bound = median - (multiplier * mad)
        upper_bound = median + (multiplier * mad)
    
    else:
        raise ValueError(f"不支持的异常值检测方法: {method}")
    
    # 应用过滤条件，但保留原始索引位置
    filtered_prices = prices.copy()
    
    # 检测并处理异常值
    for i in range(len(filtered_prices)):
        if filtered_prices[i] < lower_bound or filtered_prices[i] > upper_bound:
            # 若是异常值，使用中位数替代
            filtered_prices[i] = np.median(prices_array)
    
    return filtered_prices
```

### 4.3 DEX聚合策略
| DEX名称       | 纳入标准                        | 权重计算       | 特殊处理          |
| ----------- | --------------------------- | ---------- | ------------- |
| **Raydium** | TVL>$10,000 且 24h交易量>$1,000 | 交易量加权      | 正常纳入          |
| **Orca**    | TVL>$5,000 且 24h交易量>$500    | 交易量加权      | 正常纳入          |
| **Jupiter** | 上述任一池子 + Jupiter专有池子        | 流动性加权      | 权重提高1.2倍      |
| **小型DEX**   | TVL>$50,000 且 24h交易量>$5,000 | 固定低权重(0.5) | 价格偏离中位数>5%时排除 |
| **新上线流动性池** | 上线时间>72小时 且 TVL稳定或增长        | 低初始权重，逐日提高 | 价格波动>10%时严格审查 |

### 4.4 精度与数据质量平衡
| 优化措施               | 精度影响           | 实现难度          | 资源消耗          |
|------------------------|-------------------|-------------------|-------------------|
| 毫秒级时间戳精度       | +0.03% OHLC精度    | 低                | 存储空间增加12%    |
| 异常值处理(3σ法则)     | 极端价格±0.15%     | 中                | 计算资源增加5%     |
| 小流动性池过滤机制     | 低流动性币对±0.2%  | 高                | 开发投入较大       |
| DEX聚合算法优化        | +0.1% 价格精度     | 高                | 一次性开发成本高   |
| 开发期数据回溯测试     | 确保历史数据准确性  | 中                | 一次性计算密集     |

## 5. 持续验证框架

### 5.1 监控系统架构
```
数据流程图
Solana链上交易 → [DEX数据解析] → [交易聚合与过滤] → [K线计算] → [结果存储]
                    ↑                                      ↓
              [DEX源数据验证]                         [数据对比系统]
                                                          ↓
  [告警系统] ← [偏差检测器] → [指标系统] → [可视化面板]
```

### 5.2 自动化测试与告警
- **定期验证频率**: 每日一次完整验证，每小时抽样检查
- **告警阈值**: 任一指标平均偏差>0.1%触发告警
- **回归测试**: 每次DEX解析逻辑更新强制执行完整验证套件

### 5.3 服务级别协议(SLA)
- **精度承诺**: 99.9%的K线指标偏差≤0.1%
- **生成延迟**: 1分钟K线<3秒，小时K线<5秒
- **数据新鲜度**: 最大延迟20秒
- **可用性**: 99.99%上线率

## 6. 已知限制与未来工作
- **已知限制**:
  1. 极端市场条件下(闪崩等)的异常值处理差异
  2. 不同数据源对Solana小流动性池处理逻辑不一致
  3. 低流动性代币的价格准确性挑战
  4. 区块链重组事件处理机制差异

- **未来工作**:
  1. 探索机器学习方法预测和识别异常交易
  2. 开发自适应DEX权重算法，根据市场条件动态调整
  3. 实施更精细的时间戳同步机制
  4. 与Birdeye等服务建立规范化标准，统一行业计算逻辑

---

[[Grok research result]]
[[Deepseek research result]]
[[Gemini research result]]

角色：你是一位经验丰富的Web3 QA工程师，专精于链上数据分析和验证。你拥有区块链数据结构、时间序列数据处理和数据库优化技术的广泛知识。你熟悉ClickHouse架构及其处理大规模数据的性能特点。

输入：
- 当前数据集包含约2TB的数据（原始区块链数据和K线数据），存储在ClickHouse数据库集群中
- 需要验证并提高我们的K线数据生成过程的准确性
- 以Birdeye的K线数据作为比较基准
- 我们团队需要了解任何差异的定量差别和质性原因

步骤：
1. 深呼吸，逐步系统地分析和改进我们的K线数据。

2. 建立比较方法：
   - 定义K线验证的关键指标（开盘价、最高价、最低价、收盘价、交易量、时间戳精度）
   - 选择具有代表性的时间段和交易对进行比较（高交易量、低交易量、波动期、稳定期）
   - 创建具有统计显著性的数据比较采样策略

3. 执行初步数据比较：
   - 从我们的数据库和Birdeye的数据库中提取匹配的K线样本
   - 开发自动化比较脚本，计算所有指标的偏差百分比
   - 生成包含统计分析的综合偏差报告（误差的平均值、中位数、标准差）

4. 根本原因分析：
   - 调查我们系统与Birdeye之间的时间戳同步方法
   - 分析交易纳入标准（各系统如何决定哪些交易对K线有贡献）
   - 检查异常值处理方法（各系统如何处理极端价格变动）
   - 审查跨链资产的数据标准化技术
   - 评估在处理失败/回滚交易方面的任何差异

5. 迭代改进过程：
   - 根据对K线准确性的影响对问题进行优先级排序
   - 在测试环境中实施高优先级差异的修复
   - 每次重大更改后对照Birdeye基准重新评估准确性
   - 记录所有修改及其对准确性的测量影响

6. 性能优化：
   - 确保准确性改进不会显著降低计算速度
   - 实施适当的索引策略以加快K线生成
   - 考虑常用时间间隔的预计算方法
   - 优化ClickHouse的K线聚合查询模式

7. 验证框架：
   - 开发一个持续监控系统，定期比较我们的K线与Birdeye的K线
   - 创建偏差超过可接受限制时的警报阈值
   - 为K线生成逻辑的所有未来更改实施自动化测试

期望：
- 在所有主要指标上实现与Birdeye数据的K线数据准确度偏差在0.1%以内（一个合理的目标，应根据初步发现进行评估和调整）
- 提供所有已识别差异及其解决方案的全面文档
- 提供一个确保长期准确性的强大监控框架
- 完成一份详细报告，解释为达到目标准确性而做出的任何固有限制或权衡
- 建立明确的K线数据生成和准确性SLA，可与利益相关者沟通
- 确保改进的K线生成过程在我们的ClickHouse基础设施中保持可接受的性能

```英文原文
Role: You are an experienced Web3 QA Engineer specializing in on-chain data analysis and validation. You have extensive knowledge of blockchain data structures, time-series data processing, and database optimization techniques. You're familiar with ClickHouse's architecture and performance characteristics for large-scale data processing.

Input: 
- Current dataset consists of approximately 2TB of data (raw blockchain data and candlestick/K-line data) stored in a ClickHouse database cluster
- Need to validate and improve the accuracy of our K-line data generation process
- Birdeye's K-line data will serve as the benchmark for comparison
- Our team needs to understand both the quantitative differences and qualitative reasons for any discrepancies

Steps:
1. Take a deep breath and work step by step to systematically analyze and improve our K-line data.

2. Establish Comparison Methodology:
   - Define key metrics for K-line validation (open, high, low, close prices, volume, timestamp precision)
   - Select representative time periods and trading pairs for comparison (high volume, low volume, volatile periods, stable periods)
   - Create a statistically significant sampling strategy for data comparison

3. Perform Initial Data Comparison:
   - Extract matching K-line samples from both our database and Birdeye's database
   - Develop automated comparison scripts to calculate deviation percentages across all metrics
   - Generate comprehensive deviation reports with statistical analysis (mean, median, standard deviation of errors)

4. Root Cause Analysis:
   - Investigate timestamp synchronization methods between our system and Birdeye
   - Analyze transaction inclusion criteria (how each system decides which transactions contribute to a K-line)
   - Examine outlier handling approaches (how each system treats extreme price movements)
   - Review data normalization techniques for cross-chain assets
   - Assess any differences in handling of failed/reverted transactions

5. Iterative Improvement Process:
   - Prioritize issues based on impact on K-line accuracy
   - Implement fixes for high-priority discrepancies in a test environment
   - Re-evaluate accuracy against Birdeye benchmark after each significant change
   - Document all modifications and their measured impact on accuracy

6. Performance Optimization:
   - Ensure that accuracy improvements don't significantly degrade computation speed
   - Implement appropriate indexing strategies for faster K-line generation
   - Consider pre-computation approaches for commonly accessed time intervals
   - Optimize ClickHouse query patterns for K-line aggregation

7. Validation Framework:
   - Develop an ongoing monitoring system that regularly compares our K-lines with Birdeye's
   - Create alert thresholds for when deviation exceeds acceptable limits
   - Implement automated testing for all future changes to K-line generation logic

Expectations:
- Achieve K-line data accuracy within 0.1% deviation from Birdeye's data across all major metrics (a reasonable target that should be evaluated and adjusted based on initial findings)
- Provide comprehensive documentation of all identified discrepancies and their resolutions
- Deliver a robust monitoring framework that ensures sustained accuracy over time
- Complete a detailed report explaining any inherent limitations or tradeoffs made to achieve the target accuracy
- Establish clear SLAs for K-line data generation and accuracy that can be communicated to stakeholders
- Ensure the improved K-line generation process maintains acceptable performance within our ClickHouse infrastructure
```

